{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    classes = {'Acne': 0, 'Bags': 1, 'Redness': 2}  # Map class names to labels\n",
    "    X, y = [], []\n",
    "    for class_name, label in classes.items():\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"Folder not found: {class_path}\")\n",
    "            continue\n",
    "        for subfolder in os.listdir(class_path):  # Iterate through subfolders (0-10)\n",
    "            subfolder_path = os.path.join(class_path, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                for img_name in os.listdir(subfolder_path):  # Iterate through images\n",
    "                    img_path = os.path.join(subfolder_path, img_name)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, (128, 128))  # Resize images to 128x128\n",
    "                        X.append(img)\n",
    "                        y.append(label)\n",
    "                    else:\n",
    "                        print(f\"Failed to load image: {img_path}\")\n",
    "    print(f\"Loaded {len(X)} images from {len(classes)} classes.\")\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Dataset'\n",
    "X, y = load_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBP Feature Extraction\n",
    "def extract_lbp_features(img, P=8, R=1):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=P, R=R, method='uniform')\n",
    "    hist, _ = np.histogram(lbp, bins=np.arange(0, P+3), range=(0, P+2))\n",
    "    hist = hist.astype('float')\n",
    "    hist /= (hist.sum() + 1e-6)  # Normalize\n",
    "    return hist\n",
    "\n",
    "# HOG Feature Extraction\n",
    "def extract_hog_features(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features, _ = hog(gray, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, block_norm='L2-Hys', visualize=True)\n",
    "    return features\n",
    "\n",
    "# Color Histogram Feature Extraction\n",
    "def extract_color_features(img, h_range=(0, 128), s_range=(100, 150), v_range=(0, 128)):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    hist_h = cv2.calcHist([hsv], [0], None, [256], h_range)\n",
    "    hist_s = cv2.calcHist([hsv], [1], None, [256], s_range)\n",
    "    hist_v = cv2.calcHist([hsv], [2], None, [256], v_range)\n",
    "    hist_h /= hist_h.sum() + 1e-6\n",
    "    hist_s /= hist_s.sum() + 1e-6\n",
    "    hist_v /= hist_v.sum() + 1e-6\n",
    "    return np.concatenate([hist_h.flatten(), hist_s.flatten(), hist_v.flatten()])\n",
    "\n",
    "# GLCM Feature Extraction\n",
    "def extract_glcm_features(img, distances=[5], angles=[0], properties=('contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation')):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    glcm = graycomatrix(gray, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "    glcm_features = []\n",
    "    for prop in properties:\n",
    "        glcm_features.append(graycoprops(glcm, prop).flatten())\n",
    "    return np.concatenate(glcm_features)\n",
    "\n",
    "# Combined Feature Extraction\n",
    "def extract_combined_features(img, h_range=(0, 128), s_range=(100, 150), v_range=(0, 128)):\n",
    "    lbp_features = extract_lbp_features(img)\n",
    "    hog_features = extract_hog_features(img)\n",
    "    color_features = extract_color_features(img, h_range, s_range, v_range)\n",
    "    # glcm_features = extract_glcm_features(img)\n",
    "    return np.concatenate([lbp_features, hog_features, color_features])\n",
    "\n",
    "# Example usage with training a model\n",
    "def train_model_with_combined_features(X_images, y_labels, h_range=(0, 128), s_range=(100, 150), v_range=(0, 128)):\n",
    "    # Extract features for all images\n",
    "    X_features1 = np.array([extract_combined_features(img, h_range, s_range, v_range) for img in X_images])\n",
    "    X_features2 = np.array([extract_glcm_features(img) for img in X_images])\n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_features1, y_labels, test_size=0.2, random_state=42)\n",
    "    X2_train, X2_val, y2_train, y2_val = train_test_split(X_features2, y_labels, test_size=0.2, random_state=42)\n",
    "    # Train a classifier (SVC)\n",
    "    base_models = [\n",
    "        ('svm', SVC(kernel='linear', probability=True)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
    "    ]\n",
    "    # Meta-model\n",
    "    meta_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    \n",
    "    # Stacking Classifier\n",
    "    stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "    stacking_clf2 = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    stacking_clf2.fit(X2_train, y2_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y2_pred = stacking_clf2.predict(X2_val)\n",
    "    y_pred = stacking_clf.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracy2 = accuracy_score(y2_val, y2_pred)\n",
    "    print(f\"Validation Accuracy Gabung semua: {accuracy}\")\n",
    "    print(f\"Validation Accuracy GLCM Only: {accuracy2}\")\n",
    "    \n",
    "    return stacking_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model_with_combined_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model.pkl', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
